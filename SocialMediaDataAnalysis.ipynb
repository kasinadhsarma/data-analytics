{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean & Analyze Social Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Social media has become a ubiquitous part of modern life, with platforms such as Instagram, Twitter, and Facebook serving as essential communication channels. Social media data sets are vast and complex, making analysis a challenging task for businesses and researchers alike. In this project, we explore a simulated social media, for example Tweets, data set to understand trends in likes across different categories.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "To follow along with this project, you should have a basic understanding of Python programming and data analysis concepts. In addition, you may want to use the following packages in your Python environment:\n",
    "\n",
    "- pandas\n",
    "- Matplotlib\n",
    "- ...\n",
    "\n",
    "These packages should already be installed in Coursera's Jupyter Notebook environment, however if you'd like to install additional packages that are not included in this environment or are working off platform you can install additional packages using `!pip install packagename` within a notebook cell such as:\n",
    "\n",
    "- `!pip install pandas`\n",
    "- `!pip install matplotlib`\n",
    "\n",
    "## Project Scope\n",
    "\n",
    "The objective of this project is to analyze tweets (or other social media data) and gain insights into user engagement. We will explore the data set using visualization techniques to understand the distribution of likes across different categories. Finally, we will analyze the data to draw conclusions about the most popular categories and the overall engagement on the platform.\n",
    "\n",
    "## Step 1: Importing Required Libraries\n",
    "\n",
    "As the name suggests, the first step is to import all the necessary libraries that will be used in the project. In this case, we need pandas, numpy, matplotlib, seaborn, and random libraries.\n",
    "\n",
    "Pandas is a library used for data manipulation and analysis. Numpy is a library used for numerical computations. Matplotlib is a library used for data visualization. Seaborn is a library used for statistical data visualization. Random is a library used to generate random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.18.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (3.2.1)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.18.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (1.18.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement urllib (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for urllib\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tweepy in /opt/conda/lib/python3.7/site-packages (4.14.0)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /opt/conda/lib/python3.7/site-packages (from tweepy) (2.31.0)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy) (2.9)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy) (2020.4.5.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement csv (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for csv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install urllib\n",
    "!pip install tweepy\n",
    "!pip install csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from urllib.parse import urlparse \n",
    "import urllib\n",
    "import csv\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_fix(s, charset='utf-8'):\n",
    "    if isinstance(s, unicode):\n",
    "        s = s.encode(charset, 'ignore')\n",
    "    scheme, netloc, path, qs, anchor = urlparse.urlsplit(s)\n",
    "    path = urllib.quote(path, '/%')\n",
    "    qs = urllib.quote_plus(qs, ':&=')\n",
    "    return urlparse.urlunsplit((scheme, netloc, path, qs, anchor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tw_parser(query, location, language, tweet_type, tweet_count):\n",
    "    global qw, ge, l, t, c, d\n",
    "\n",
    "# USE EXAMPLES:\n",
    "# =-=-=-=-=-=-=\n",
    "# % twsearch <search term>            --- searches term\n",
    "# % twsearch <search term> -g sf      --- searches term in SF geographic box <DEFAULT = none>\n",
    "# % twsearch <search term> -l en      --- searches term with lang=en (English) <DEFAULT = en>\n",
    "# % twsearch <search term> -t {m,r,p} --- searches term of type: mixed, recent, or popular <DEFAULT = recent>\n",
    "# % twsearch <search term> -c 12      --- searches term and returns 12 tweets (count=12) <DEFAULT = 1>\n",
    "# % twsearch <search term> -o {ca, tx, id, co, rtc)   --- searches term and sets output options <DEFAULT = ca, tx>\n",
    "\n",
    "# Parse the command\n",
    "    #parser = argparse.ArgumentParser(description='Twitter Search')\n",
    "    #parser.add_argument(action='store', dest='query', help='Search term string')\n",
    "    #parser.add_argument('-g', action='store', dest='loca', help='Location (lo, nyl, nym, nyu, dc, sf, nb')\n",
    "    #parser.add_argument('-l', action='store', dest='l', help='Language (en = English, fr = French, etc...)')\n",
    "    #parser.add_argument('-t', action='store', dest='t', help='Search type: mixed, recent, or popular')\n",
    "    #parser.add_argument('-c', action='store', dest='c', help='Tweet count (must be <50)')\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    qw = query     # Actual query word(s)\n",
    "    ge = ''\n",
    "\n",
    "    # Location\n",
    "    loca = location\n",
    "    if (not(loca in ('lo', 'nyl', 'nym', 'nyu', 'dc', 'sf', 'nb')) and (loca)):\n",
    "        print (\"WARNING: Location must be one of these: lo, nyl, nym, nyu, dc, sf, nb\")\n",
    "        #exit()\n",
    "    if loca:\n",
    "        ge = locords[loca]\n",
    "\n",
    "    # Language\n",
    "    l = language\n",
    "    if (not l):\n",
    "        l = \"en\"\n",
    "    if (not(l in ('en','fr','es','po','ko', 'ar'))):\n",
    "        print (\"WARNING: Languages currently supported are: en (English), fr (French), es (Spanish), po (Portuguese), ko (Korean), ar (Arabic)\")\n",
    "        #exit()\n",
    "\n",
    "    # Tweet type\n",
    "    t = tweet_type\n",
    "    if (not t):\n",
    "        t = \"recent\"\n",
    "    if (not(t in ('mixed','recent','popular'))):\n",
    "        print (\"WARNING: Search type must be one of: (m)ixed, (r)ecent, or (p)opular\")\n",
    "        #exit()\n",
    "\n",
    "    # Tweet count\n",
    "    if tweet_count:\n",
    "        c = int(tweet_count)\n",
    "        if (c > cmax):\n",
    "            print (\"Resetting count to \",cmax,\" (maximum allowed)\")\n",
    "            c = cmax\n",
    "        if (not (c) or (c < 1)):\n",
    "            c = 1\n",
    "    if not(c):\n",
    "        c = 1\n",
    "\n",
    "    print (\"Query: %s, Location: %s, Language: %s, Search type: %s, Count: %s\" %(qw,ge,l,t,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tw_oauth(authfile):\n",
    "    with open(authfile, \"r\") as f:\n",
    "        ak = f.readlines()\n",
    "    f.close()\n",
    "    auth1 = tweepy.auth.OAuthHandler(ak[0].replace(\"\\n\",\"\"), ak[1].replace(\"\\n\",\"\"))\n",
    "    auth1.set_access_token(ak[2].replace(\"\\n\",\"\"), ak[3].replace(\"\\n\",\"\"))\n",
    "    return tweepy.API(auth1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tw_search_json(query, cnt=5):\n",
    "    authfile = './auth.k'\n",
    "    api = tw_oauth(authfile)\n",
    "    results = {}\n",
    "    meta = {\n",
    "        'username': 'text',\n",
    "        'usersince': 'date',\n",
    "        'followers': 'numeric',\n",
    "        'friends': 'numeric',\n",
    "        'authorid': 'text',\n",
    "        'authorloc': 'geo',\n",
    "        'geoenable': 'boolean',\n",
    "        'source': 'text'\n",
    "    }\n",
    "    data = []\n",
    "    for tweet in tweepy.Cursor(api.search, q=query, count=cnt).items():\n",
    "        dTwt = {}\n",
    "        dTwt['username'] = tweet.author.name\n",
    "        dTwt['usersince'] = tweet.author.created_at      #author/user profile creation date\n",
    "        dTwt['followers'] = tweet.author.followers_count #number of author/user followers (inlink)\n",
    "        dTwt['friends']   = tweet.author.friends_count   #number of author/user friends (outlink)\n",
    "        dTwt['authorid']  = tweet.author.id              #author/user ID#\n",
    "        dTwt['authorloc'] = tweet.author.location        #author/user location\n",
    "        dTwt['geoenable'] = tweet.author.geo_enabled     #is author/user account geo enabled?\n",
    "        dTwt['source']    = tweet.source                 #platform source for tweet\n",
    "        data.append(dTwt)\n",
    "    results['meta'] = meta\n",
    "    results['data'] = data\n",
    "    return results\n",
    "\n",
    "def tw_search(api):\n",
    "    counter = 0\n",
    "    # Open/Create a file to append data\n",
    "    csvFile = open('twitter_result.csv','w')\n",
    "    #Use csv Writer\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    csvWriter.writerow([\"created\", \"text\", \"retwc\", \"hashtag\", \"followers\", \"friends\"])\n",
    "\t\n",
    "    for tweet in tweepy.Cursor(api.search,\n",
    "                                q = qw,\n",
    "                                g = ge,\n",
    "                                lang = l,\n",
    "                                result_type = t,\n",
    "                                count = c).items():\n",
    "\n",
    "        #TWEET INFO\n",
    "        created = tweet.created_at   #tweet created\n",
    "        text    = tweet.text         #tweet text\n",
    "        tweet_id = tweet.id          #tweet ID# (not author ID#)\n",
    "        cords   = tweet.coordinates  #geographic co-ordinates\n",
    "        retwc   = tweet.retweet_count #re-tweet count\n",
    "        try:\n",
    "            hashtag = tweet.entities[u'hashtags'][0][u'text'] #hashtags used\n",
    "        except:\n",
    "            hashtag = \"None\"\n",
    "        try:\n",
    "            rawurl = tweet.entities[u'urls'][0][u'url'] #URLs used\n",
    "            urls = url_fix(rawurl)\n",
    "        except:\n",
    "            urls    = \"None\"\n",
    "        #AUTHOR INFO\n",
    "        username  = tweet.author.name            #author/user name\n",
    "        usersince = tweet.author.created_at      #author/user profile creation date\n",
    "        followers = tweet.author.followers_count #number of author/user followers (inlink)\n",
    "        friends   = tweet.author.friends_count   #number of author/user friends (outlink)\n",
    "        authorid  = tweet.author.id              #author/user ID#\n",
    "        authorloc = tweet.author.location        #author/user location\n",
    "        #TECHNOLOGY INFO\n",
    "        geoenable = tweet.author.geo_enabled     #is author/user account geo enabled?\n",
    "        source    = tweet.source                 #platform source for tweet\n",
    "\t\t# Dongho 03/28/16\n",
    "        csvWriter.writerow([created, str(text).encode(\"utf-8\"), retwc, hashtag, followers, friends])\n",
    "        counter = counter +1\n",
    "        if (counter == c):\n",
    "            break\n",
    "\n",
    "    csvFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: game of thrones, Location: 0, 51.503, 20km, Language: en, Search type: recent, Count: 50\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './auth.k'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-41a8eb7c36b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-41a8eb7c36b4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtw_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'game of thrones'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'recent'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtw_oauth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mtw_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-bb909c1b8453>\u001b[0m in \u001b[0;36mtw_oauth\u001b[0;34m(authfile)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtw_oauth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mak\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mauth1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOAuthHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mak\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './auth.k'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    global api, cmax, locords\n",
    "\n",
    "    # Geo-coordinates of five metropolitan areas\n",
    "    # London, NYC (lower, middle, upper), Wash DC, San Francisco, New Brunswick (NJ)\n",
    "    locords =  {'lo': '0, 51.503, 20km',\n",
    "                'nyl': '-74, 40.73, 2mi',\n",
    "                'nym': '-74, 40.74, 2mi',\n",
    "                'nyu': '-73.96, 40.78, 2mi',\n",
    "                'dc': '-77.04, 38.91, 2mi',\n",
    "                'sf': '-122.45, 37.74, 5km',\n",
    "                'nb': '-74.45, 40.49, 2mi'}\n",
    "    # Maximum allowed tweet count (note: Twitter sets this to ~180 per 15 minutes)\n",
    "    cmax = 50\n",
    "    # OAuth key file\n",
    "    authfile = './auth.k'\n",
    "\n",
    "    tw_parser('game of thrones','lo','en','recent',50)\n",
    "    api = tw_oauth(authfile)\n",
    "    tw_search(api)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
